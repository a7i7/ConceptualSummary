import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations.CollapsedCCProcessedDependenciesAnnotation;
import edu.stanford.nlp.dcoref.CorefCoreAnnotations.CorefChainAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.trees.GrammaticalStructure;
import edu.stanford.nlp.trees.GrammaticalStructureFactory;
import edu.stanford.nlp.trees.PennTreebankLanguagePack;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.trees.TreebankLanguagePack;
import edu.stanford.nlp.trees.TypedDependency;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.dcoref.CorefChain;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.util.CoreMap;
import java.io.IOException;
import java.util.Collection;
import java.util.List;
import java.util.Map;

import java.util.Properties;

public class JustRandomTests {
//
///**
// * @param args the command line arguments
// */
//
//	private static void getDependencyTree(Tree tree)
//	{
//		System.out.println("FUCK START");
//		TreebankLanguagePack tlp = new PennTreebankLanguagePack();
//		GrammaticalStructureFactory gsf = tlp.grammaticalStructureFactory();
//		GrammaticalStructure gs = gsf.newGrammaticalStructure(tree);
//		Collection<TypedDependency> td = gs.typedDependenciesCollapsed();
//		System.out.println(td);
//
//		Object[] list = td.toArray();
//		System.out.println(list.length);
//		TypedDependency typedDependency;
//		for (Object object : list) {
//		typedDependency = (TypedDependency) object;
//		System.out.println("Depdency Name"+typedDependency.dep().nodeString()+ " :: "+ "Node"+typedDependency.reln());
////		if (typedDependency.reln().getShortName().equals("something")) {
////		   //your code
//		System.out.println(typedDependency);
//		}
//		System.out.println("FUCK END");
//		System.out.println("~~~~~~~~~~~~~~~~~~~~~~~~");
////		System.exit(0);
//	}
//public static void main(String[] args) throws IOException, ClassNotFoundException {
//	
//	WordList wordList = new WordList("");
//	wordList.setSentence("cricket football cricket hello volley cricket golf golf yo fuck");
//	String str = "Enormous increasing and easy availability of information on the World Wide Web have recently resulted in brushing up the classical linguistics problem - the condensation of information from text documents. This task is essentially a data reduction process. It was manually exerted from time out of mind and firstly computerized in late 50th . Resulted summary has to inform by selection and or by generalization on important content and conclusions in the original text. Recent scientific knowledge and more efficient computers form a new challenge giving the chace to solve the information overload problem or at least to postpone its decision and decrease its negative impact. I ave , a cat. Motha fucka ! YO?? ";
//	List<String> sentences = TextSplitter.splitIntoSentences(str);
//	for(String sad:sentences)
//		System.out.println(sad);
//	List<String> temp = TextSplitter.splitIntoWords("Hi, David. What's up? I love your jacket");
//	for(String w:temp)
//		System.out.println(w);
//	System.exit(0);
//	
//    
//	// creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
//    Properties props = new Properties();
//    props.put("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
//    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
//
//    // read some text in the text variable
//    String text = "Afif has a Porsche. It is red in color. Porsche has manual transmission."; // Add your text here!
//
//    // create an empty Annotation just with the given text
//    Annotation document = new Annotation(text);
//
//    // run all Annotators on this text
//    pipeline.annotate(document);
//
//    // these are all the sentences in this document
//    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
//    List<CoreMap> sentences = document.get(SentencesAnnotation.class);
//
//    for(CoreMap sentence: sentences) {
//      // traversing the words in the current sentence
//      // a CoreLabel is a CoreMap with additional token-specific methods
//      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
//        // this is the text of the token
//        String word = token.get(TextAnnotation.class);
//        // this is the POS tag of the token
//        String pos = token.get(PartOfSpeechAnnotation.class);
//        // this is the NER label of the token
//        String ne = token.get(NamedEntityTagAnnotation.class);  
//        
//        System.out.println(word+" :: "+pos+" :: "+ne);
//      }
//
//      // this is the parse tree of the current sentence
//      Tree tree = sentence.get(TreeAnnotation.class);
//      System.out.println(tree);
//      getDependencyTree(tree);
//      // this is the Stanford dependency graph of the current sentence
////      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
////      System.out.println(dependencies.toList());
//      
//    }
//    System.exit(0);
//    // This is the coreference link graph
//    // Each chain stores a set of mentions that link to each other,
//    // along with a method for getting the most representative mention
//    // Both sentence and token offsets start at 1!
//    Map<Integer, CorefChain> graph = 
//      document.get(CorefChainAnnotation.class);
//    
//    for(Integer idx:graph.keySet())
//    {
//    	System.out.println(idx);
//        CorefChain c = graph.get(idx);
//        System.out.println(c.getRepresentativeMention());
//    }
//    System.out.println(graph);
//  }
}